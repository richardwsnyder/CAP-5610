{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "problem3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardwsnyder/CAP-5610/blob/master/problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YnVCSBB8NZv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7SaFBDCrNf87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the training and test data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_train = x_train.astype('float32') / 255 \n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Convert the labels into vectors of size 10\n",
        "# where the index of the label is 1\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OY8euoUNiBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weight matrix of size 784 rows for each image \n",
        "# by 10 columns for each z node\n",
        "weight = np.zeros((784, 10))\n",
        "# manipulable weight matrix\n",
        "w = weight\n",
        "\n",
        "# bias vector or size 10 for each z node\n",
        "bias = np.random.rand(10, 1)\n",
        "b = bias\n",
        "\n",
        "# seed random number generator\n",
        "np.random.seed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYJm_xRINkGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Softmax takes in a vector z that contains the \n",
        "# calculated z values for all 10 z nodes\n",
        "# This function will return a 10x1 vector, which will be stored\n",
        "# as activation functions for each z node\n",
        "def softmax(z):\n",
        "    # find the maximum value of the z vector\n",
        "    z1 = z.max()\n",
        "    # subtract that maximum value from each entry in the \n",
        "    # vector and apply the exponential function to each entry\n",
        "    z2 = np.exp(z - z1)\n",
        "    return z2 / z2.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDTjU2LHNmVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Categorical cross entropy loss function\n",
        "# Take in a vector y (one hot encodings) and a vector z\n",
        "# Apply exponential function to each item of z and take sum over \n",
        "# those values. Then take the log of that calculated value. Subtract that value\n",
        "# from each entry of z as to not have underflow.\n",
        "def catCrossEnt(y, z):\n",
        "    return  -1 * np.sum(y.T.dot(z - np.log(np.exp(z).sum())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2zgtLmPQNn64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Categorical cross entropy derivative with respect to w_ji\n",
        "# takes in an image x, a vector y (one hot encodings),\n",
        "# and a vector a (activation values)\n",
        "# Returns a 784 x 10 matrix of weighted values \n",
        "def partialLwrtW(x, y, a):\n",
        "    dz = a - y.T \n",
        "    return x.T.dot(dz.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCwf2W0FNprq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Categorical cross entropy derivative with respect to b_j\n",
        "# Returns a 10 x 1 vector of biases\n",
        "def partialLwrtB(y, a):\n",
        "    return np.sum(a - y.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGMR3-h_Nrp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train the model based off of stochastic gradient descent\n",
        "def train_model(epochs, learning_rate):\n",
        "    global w\n",
        "    global b\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        s = np.random.permutation(60000)\n",
        "        x_shuffled = x_train[s]\n",
        "        y_shuffled = y_train[s]\n",
        "        for i in range(60000):\n",
        "            x = x_shuffled[i].reshape(1, 784)\n",
        "            y = y_shuffled[i].reshape(1, 10)\n",
        "            # this will make z a 10x1 vector\n",
        "            z = w.T.dot(x.T) + b \n",
        "            a = softmax(z)\n",
        "            dw = partialLwrtW(x, y, a)\n",
        "            db = partialLwrtB(y, a).T\n",
        "            w = w - (dw * learning_rate) \n",
        "            b = b - (db * learning_rate) \n",
        "            \n",
        "        print(\"Epoch %d completed\" % epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iOMxHEvoNuEJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use the newly created model to test accuracy on new\n",
        "# test data\n",
        "def test_model():\n",
        "    global w\n",
        "    global b\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    for i in range(10000):\n",
        "        x = x_test[i].reshape(784, 1)\n",
        "        y = y_test[i].reshape(10, 1)\n",
        "        z = w.T.dot(x) + b \n",
        "        a = softmax(z)\n",
        "        loss += catCrossEnt(y, z) \n",
        "        if np.argmax(a) == np.argmax(y):\n",
        "            correct += 1\n",
        "\n",
        "    print(\"Accuracy - %0.4f percent\" % ((correct / 10000) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPqWCI5BNvsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "349e092a-08af-4606-e5da-6d6bbde06bab"
      },
      "cell_type": "code",
      "source": [
        "train_model(10, 0.01)\n",
        "test_model()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 completed\n",
            "Epoch 1 completed\n",
            "Epoch 2 completed\n",
            "Epoch 3 completed\n",
            "Epoch 4 completed\n",
            "Epoch 5 completed\n",
            "Epoch 6 completed\n",
            "Epoch 7 completed\n",
            "Epoch 8 completed\n",
            "Epoch 9 completed\n",
            "Accuracy - 91.4200 percent\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}